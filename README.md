# MLOps Guardian Agent  
_Watcher â†’ Diagnoser â†’ Remediator_

A fullyâ€‘local, multiâ€‘agent watchdog that keeps your ML model healthy:

| Stage        | Purpose                                                                                   |
|--------------|-------------------------------------------------------------------------------------------|
| **Watcher**  | Detect new evaluation logs in `data/logs/` and enqueue them                               |
| **Diagnoser**| Run **Evidently** + LLM reasoning to label each run **OK / DRIFT / REGRESSION**            |
| **Remediator**| Decide **retrain**, **rollback**, or **noop** and execute the action                     |

All three agents run as independent Python processes and talk through `multiprocessing.Queue`.
Swap the LLM backend (OpenAI, Ollama, llamaâ€‘cpp, etc.) without touching the control flow.

---

## Project layout

```
mlops-guardian-agent/
â”œâ”€â”€ agent/              # all three agents + shared tools
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ monitors.py
â”‚   â”œâ”€â”€ tools.py
â”‚   â”œâ”€â”€ watcher.py
â”‚   â”œâ”€â”€ diagnoser.py
â”‚   â”œâ”€â”€ remediator.py
â”‚   â””â”€â”€ orchestrator.py   â† entryâ€‘point
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ training_pipeline.py
â”œâ”€â”€ app/                # optional FastAPI service
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ ref/            # ONE reference CSV goes here
â”‚   â””â”€â”€ logs/           # new eval logs appear here
â”œâ”€â”€ models/             # latest.pkl etc.
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_guardian.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

---

## Quickâ€‘start (local, with OpenAI API)

```bash
git clone https://github.com/yourname/mlops-guardian-agent.git
cd mlops-guardian-agent

python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# --- configure ---
cp .env.example .env
# edit .env  â†’ add your OPENAI_API_KEY

# --- seed reference data ---
mkdir -p data/ref
python - <<'PY'
import pandas as pd, pathlib
df = pd.DataFrame({"f1":[0,1], "label":[0,1]})
df.to_csv("data/ref/ref.csv", index=False)
PY

# --- launch agents (three processes) ---
python -m agent.orchestrator
```

Open a second terminal and create a new evaluation log:

```bash
source .venv/bin/activate
python pipelines/training_pipeline.py
```

Youâ€™ll see log lines from **Watcher â†’ Diagnoser â†’ Remediator** along with the chosen action.

---

## Offline / selfâ€‘hosted LLM

1. Install [Ollama](https://ollama.com/) and pull a model:

   ```bash
   ollama pull llama3
   ```

2. In `.env` set  
   `MODEL_PATH=ollama/llama3`

3. Replace the import in `agent/diagnoser.py`, `agent/remediator.py`, and `app/main.py`:

   ```python
   # from langchain.chat_models import ChatOpenAI
   # llm = ChatOpenAI(...)
   from langchain.chat_models import ChatOllama
   llm = ChatOllama(model="llama3")
   ```

Everything else stays the sameâ€”no internet required.

---

## REST API (optional)

```bash
uvicorn app.main:app --reload
```

| Method & path   | Body (JSON)                                  | Description                       |
|-----------------|----------------------------------------------|-----------------------------------|
| `POST /inspect/`| `{"ref":"path.csv","cur":"path.csv"}`         | Adâ€‘hoc drift check (Evidently)    |
| `POST /action/` | `{"kind":"retrain" \| "rollback"}`            | Manual override of Remediator     |

---

## Running tests

```bash
pytest
```

The smokeâ€‘test simply imports the orchestrator and pipeline to verify dependencies.

---

## Docker

```bash
docker build -t guardian .
docker run -it -v $(pwd)/data:/app/data guardian
```

---

## Customizing

* **Drift / accuracy thresholds** â€” change `DRIFT_THRESHOLD` in `.env` and the `0.92` cutoff inside `agent/diagnoser.py`.
* **Retraining command** â€” set `RETRAIN_COMMAND` in `.env`.
* **Log format** â€” adapt `agent/monitors.py` for regression, NLP, etc.
* **Message bus** â€” swap `multiprocessing.Queue` for Redis, Kafka, or Prefect Orion to distribute across machines.

---

## License

MIT â€” do with it what you like. If you build something cool, let me know! ğŸ‰
